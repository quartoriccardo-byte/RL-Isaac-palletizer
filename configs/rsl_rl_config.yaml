
seed: 1
runner_class_name: OnPolicyRunner
policy_class_name: PalletizerActorCritic
algorithm_class_name: PPO

policy:
  init_noise_std: 1.0
  actor_hidden_dims: [512, 256, 128]
  critic_hidden_dims: [512, 256, 128]
  activation: 'elu'
  # PalletizerActorCritic specific args
  num_actor_obs: 4120 # 4096 (Img) + 24 (Proprio)
  num_critic_obs: 4120
  num_actions: 4

algorithm:
  value_loss_coef: 1.0
  use_clipped_value_loss: True
  clip_param: 0.2
  entropy_coef: 0.01
  num_learning_epochs: 1500
  num_mini_batches: 4 # For 4096 envs
  learning_rate: 1.0e-3
  schedule: 'adaptive'
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.01
  max_grad_norm: 1.0

runner:
  policy_class_name: 'PalletizerActorCritic'
  algorithm_class_name: 'PPO'
  num_steps_per_env: 24
  max_iterations: 1500
  save_interval: 50
  experiment_name: 'palletizing_rsl_rl'
  run_name: 'warp_unet_v1'
  resume: False
  load_run: -1
  checkpoint: -1
