
seed: 1
runner_class_name: OnPolicyRunner
policy_class_name: PalletizerActorCritic
algorithm_class_name: PPO

policy:
  init_noise_std: 1.0
  actor_hidden_dims: [512, 256, 128]
  critic_hidden_dims: [512, 256, 128]
  activation: 'elu'

algorithm:
  value_loss_coef: 1.0
  use_clipped_value_loss: True
  clip_param: 0.2
  entropy_coef: 0.01
  num_learning_epochs: 1500
  num_mini_batches: 4
  learning_rate: 1.0e-3
  schedule: 'adaptive'
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.01
  max_grad_norm: 1.0

runner:
  policy_class_name: 'PalletizerActorCritic'
  algorithm_class_name: 'PPO'
  num_steps_per_env: 24 # 24 * 4096 = 98304 per rollout
  max_iterations: 1500
  save_interval: 50
  experiment_name: 'palletizing_ppo'
  run_name: ''
  resume: False
  load_run: -1
  checkpoint: -1
